{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f8a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from dataset import DrumPatternDataset\n",
    "from hierarchical_vae import HierarchicalDrumVAE\n",
    "from training_utils import kl_annealing_schedule, temperature_annealing_schedule\n",
    "from train import compute_hierarchical_elbo\n",
    "from visualize import plot_drum_pattern\n",
    "from analyze_latent import (\n",
    "    visualize_latent_hierarchy, interpolate_styles,\n",
    "    measure_disentanglement, controllable_generation\n",
    ")\n",
    "\n",
    "# Paths\n",
    "results_dir = Path(\"results\")\n",
    "generated_dir = results_dir / \"generated_patterns\"\n",
    "latent_dir = results_dir / \"latent_analysis\"\n",
    "\n",
    "for d in [generated_dir, latent_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572b6389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kangsung/anaconda3/envs/peft/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/kangsung/anaconda3/envs/peft/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss=99.7768, Recon=99.7768, KL_low=676.1357, KL_high=1386.4526, Beta=0.000, Temp=2.00\n",
      "Epoch 0 Validation - Loss: 36094.6738 KL_high: 34735.4806 KL_low: 1259.4785 Validity: 0.968 Diversity: 0.085\n",
      "Epoch 10, Loss=98.9621, Recon=98.9553, KL_low=0.0206, KL_high=0.0131, Beta=0.200, Temp=1.85\n",
      "Epoch 10 Validation - Loss: 98.9123 KL_high: 0.0171 KL_low: 0.0237 Validity: 0.881 Diversity: 0.294\n",
      "Epoch 20, Loss=99.0737, Recon=98.9184, KL_low=0.3805, KL_high=0.0080, Beta=0.400, Temp=1.70\n",
      "Epoch 20 Validation - Loss: 98.9863 KL_high: 0.0089 KL_low: 0.4154 Validity: 0.936 Diversity: 0.273\n",
      "Epoch 30, Loss=98.9551, Recon=98.7375, KL_low=0.3578, KL_high=0.0048, Beta=0.600, Temp=1.55\n",
      "Epoch 30 Validation - Loss: 98.8856 KL_high: 0.0052 KL_low: 0.3241 Validity: 0.976 Diversity: 0.172\n",
      "Epoch 40, Loss=99.2280, Recon=99.0483, KL_low=0.2212, KL_high=0.0035, Beta=0.800, Temp=1.40\n",
      "Epoch 40 Validation - Loss: 98.9007 KL_high: 0.0035 KL_low: 0.2190 Validity: 0.963 Diversity: 0.205\n",
      "Epoch 50, Loss=98.5480, Recon=98.4112, KL_low=0.1337, KL_high=0.0031, Beta=1.000, Temp=1.25\n",
      "Epoch 50 Validation - Loss: 98.7190 KL_high: 0.0035 KL_low: 0.1428 Validity: 0.976 Diversity: 0.189\n",
      "Epoch 60, Loss=98.7708, Recon=98.5885, KL_low=0.1792, KL_high=0.0031, Beta=1.000, Temp=1.10\n",
      "Epoch 60 Validation - Loss: 98.7658 KL_high: 0.0035 KL_low: 0.1522 Validity: 0.983 Diversity: 0.188\n",
      "Epoch 70, Loss=99.6347, Recon=99.4706, KL_low=0.1610, KL_high=0.0031, Beta=1.000, Temp=0.95\n",
      "Epoch 70 Validation - Loss: 98.8436 KL_high: 0.0035 KL_low: 0.1547 Validity: 0.984 Diversity: 0.163\n",
      "Epoch 80, Loss=98.5810, Recon=98.4689, KL_low=0.1089, KL_high=0.0031, Beta=1.000, Temp=0.80\n",
      "Epoch 80 Validation - Loss: 98.8067 KL_high: 0.0035 KL_low: 0.1371 Validity: 0.984 Diversity: 0.177\n",
      "Epoch 90, Loss=99.2648, Recon=99.1409, KL_low=0.1208, KL_high=0.0031, Beta=1.000, Temp=0.65\n",
      "Epoch 90 Validation - Loss: 98.7729 KL_high: 0.0035 KL_low: 0.1331 Validity: 0.974 Diversity: 0.199\n",
      "Training complete. Results saved to results/\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "\n",
    "from train import main as train_main\n",
    "train_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff01acda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10 samples per style to results/generated_patterns\n"
     ]
    }
   ],
   "source": [
    "# Generate and Save 10 Samples Per Style\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# load the trained model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalDrumVAE(\n",
    "    z_high_dim=4,\n",
    "    z_low_dim=12\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"results/best_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "gen_dir = Path(\"results/generated_patterns\")\n",
    "gen_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate samples\n",
    "n_samples_per_style = 10\n",
    "all_generated = {s: [] for s in range(5)}  # 5 styles\n",
    "\n",
    "# Val Loader\n",
    "data_dir = \"../data/drums\"\n",
    "val_dataset = DrumPatternDataset(data_dir, split=\"val\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for patterns, styles, densities in val_loader:\n",
    "        patterns = patterns.to(device)\n",
    "        out = model(patterns)\n",
    "        recon = out['recon']\n",
    "        sampled = (torch.sigmoid(recon) > 0.5).float().cpu()\n",
    "\n",
    "        for p, s in zip(sampled, styles):\n",
    "            s = int(s.item())\n",
    "            if len(all_generated[s]) < n_samples_per_style:\n",
    "                all_generated[s].append(p)\n",
    "\n",
    "        if all(len(v) >= n_samples_per_style for v in all_generated.values()):\n",
    "            break\n",
    "\n",
    "# Save results\n",
    "style_names = [\"Rock\", \"Jazz\", \"Hip-hop\", \"Electronic\", \"Latin\"]\n",
    "\n",
    "for s, patterns in all_generated.items():\n",
    "    for idx, p in enumerate(patterns):\n",
    "        fig = plot_drum_pattern(p, title=f\"Style {style_names[s]} - Sample {idx}\")\n",
    "        fig.savefig(gen_dir / f\"style{s}_{style_names[s]}_sample{idx}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "print(f\"Saved 10 samples per style to {gen_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c5d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved interpolation sequence to results/generated_patterns\n"
     ]
    }
   ],
   "source": [
    "# Latent Space Interpolation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "interp_dir = Path(\"results/generated_patterns\")\n",
    "interp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Take two random validation samples\n",
    "patterns_a, style_a, _ = val_dataset[0]\n",
    "patterns_b, style_b, _ = val_dataset[1]\n",
    "\n",
    "patterns_a = patterns_a.unsqueeze(0).to(device)\n",
    "patterns_b = patterns_b.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_a = model.encode_hierarchy(patterns_a)\n",
    "    z_low_a, _, _, z_high_a, _, _ = out_a\n",
    "\n",
    "    out_b = model.encode_hierarchy(patterns_b)\n",
    "    z_low_b, _, _, z_high_b, _, _ = out_b\n",
    "\n",
    "    # Interpolate\n",
    "    n_steps = 10\n",
    "    z_low_interp = [z_low_a * (1 - t) + z_low_b * t for t in np.linspace(0, 1, n_steps)]\n",
    "    z_high_interp = [z_high_a * (1 - t) + z_high_b * t for t in np.linspace(0, 1, n_steps)]\n",
    "\n",
    "    # Decode interpolated latents\n",
    "    for idx, (zl, zh) in enumerate(zip(z_low_interp, z_high_interp)):\n",
    "        recon = model.decode_hierarchy(zl, zh)\n",
    "        sampled = (torch.sigmoid(recon) > 0.5).float().cpu()[0]\n",
    "\n",
    "        fig = plot_drum_pattern(\n",
    "            sampled, \n",
    "            title=f\"Interpolation Step {idx}\"\n",
    "        )\n",
    "        fig.savefig(interp_dir / f\"interpolation_step{idx}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "print(f\"Saved interpolation sequence to {interp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bae5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved style transfer results to results/generated_patterns\n"
     ]
    }
   ],
   "source": [
    "# Style Transfer Examples\n",
    "\n",
    "transfer_dir = Path(\"results/generated_patterns\")\n",
    "transfer_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.eval()\n",
    "style_names = [\"Rock\", \"Jazz\", \"Hip-hop\", \"Electronic\", \"Latin\"]\n",
    "\n",
    "n_styles = len(style_names)\n",
    "sources = [val_dataset[i][0].unsqueeze(0).to(device) for i in range(n_styles)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for src_idx, src_pattern in enumerate(sources):\n",
    "        z_low, mu_low, logvar_low, z_high, mu_high, logvar_high = model.encode_hierarchy(src_pattern)\n",
    "\n",
    "        for tgt_idx in range(n_styles):\n",
    "            tgt_pattern = val_dataset[tgt_idx][0].unsqueeze(0).to(device)\n",
    "            _, _, _, z_high_tgt, _, _ = model.encode_hierarchy(tgt_pattern)\n",
    "\n",
    "            recon = model.decode_hierarchy(z_low, z_high_tgt)\n",
    "            sampled = (torch.sigmoid(recon) > 0.5).float().cpu()[0]\n",
    "\n",
    "            fig = plot_drum_pattern(\n",
    "                sampled,\n",
    "                title=f\"{style_names[src_idx]} → {style_names[tgt_idx]}\"\n",
    "            )\n",
    "            fig.savefig(\n",
    "                transfer_dir / f\"{style_names[src_idx]}_to_{style_names[tgt_idx]}.png\"\n",
    "            )\n",
    "            plt.close(fig)\n",
    "\n",
    "print(f\"Saved style transfer results to {transfer_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae16a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved latent space visualization to results/latent_analysis\n"
     ]
    }
   ],
   "source": [
    "# Latent Space Visualization with t-SNE\n",
    "\n",
    "from pathlib import Path\n",
    "from visualize import *\n",
    "\n",
    "latent_dir = Path(\"results/latent_analysis\")\n",
    "latent_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_latents = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for patterns, styles, _ in val_loader:\n",
    "        patterns = patterns.to(device)\n",
    "        z_low, mu_low, logvar_low, z_high, mu_high, logvar_high = model.encode_hierarchy(patterns)\n",
    "        \n",
    "        all_latents.append(z_high.cpu())\n",
    "        all_labels.append(styles)\n",
    "\n",
    "all_latents = torch.cat(all_latents, dim=0).numpy()\n",
    "all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "fig = plot_latent_space_2d(all_latents, labels=all_labels, title=\"Latent Space (z_high)\")\n",
    "fig.savefig(latent_dir / \"latent_space_tsne.png\")\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"Saved latent space visualization to {latent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1bed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved latent t-SNE plots to results/latent_analysis\n"
     ]
    }
   ],
   "source": [
    "# Latent Analysis: t-SNE\n",
    "\n",
    "from pathlib import Path\n",
    "from analyze_latent import visualize_latent_hierarchy, plot_kl_trends\n",
    "\n",
    "latent_dir = Path(\"results/latent_analysis\")\n",
    "latent_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "z_high, z_low, labels = visualize_latent_hierarchy(model, val_loader, device=device)\n",
    "print(f\"Saved latent t-SNE plots to {latent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256031ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved humanization results to results/generated_patterns\n"
     ]
    }
   ],
   "source": [
    "# Humanization (Variation Injection)\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HierarchicalDrumVAE(\n",
    "    z_high_dim=4,\n",
    "    z_low_dim=12\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"results/best_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "humanize_dir = Path(\"results/generated_patterns\")\n",
    "humanize_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Style\n",
    "style_names = [\"Rock\", \"Jazz\", \"Hip-hop\", \"Electronic\", \"Latin\"]\n",
    "\n",
    "n_variations = 5  \n",
    "with torch.no_grad():\n",
    "    for style_idx, style_name in enumerate(style_names):\n",
    "        # fix z_high\n",
    "        z_high = torch.randn(1, model.z_high_dim).to(device)\n",
    "\n",
    "        for v in range(n_variations):\n",
    "            # inject random variation into z_low\n",
    "            z_low = torch.randn(1, model.z_low_dim).to(device)\n",
    "            logits = model.decode_hierarchy(z_high, z_low)\n",
    "            pattern = (torch.sigmoid(logits) > 0.5).cpu().squeeze(0)\n",
    "\n",
    "            # Save visualization\n",
    "            fig = plot_drum_pattern(pattern, title=f\"{style_name} - Humanized {v}\")\n",
    "            fig.savefig(humanize_dir / f\"{style_name}_variation{v}.png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "print(f\"Saved humanization results to {humanize_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
